{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import seaborn as sns\n",
    "    # Seaborn style (figure aesthetics only)\n",
    "    sns.set(context='paper', style='whitegrid', font_scale=1.2)\n",
    "    sns.set_style('ticks', {'xtick.direction':'in', 'ytick.direction':'in'})\n",
    "except ImportError:\n",
    "    print('Seaborn not installed. Going without it.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "from sklearn.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import uniform"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PV Data\n",
    "\n",
    "5 seconds resolution MiRIS PV from 13/05/2019 to 21/06/2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv = pd.read_csv('miris_pv.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling the dataset from 5-seconds to 15-minutes resolution (using mean)\n",
    "pv = pv.resample('15min').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15-minute resolution weather data\n",
    "\n",
    "The file is composed of forecast of several weather variables:\n",
    "\n",
    "    CD = low clouds (0 to 1)\n",
    "    CM = medium clouds (0 to 1)\n",
    "    CU = high clouds (0 to 1)\n",
    "    PREC = precipitation (mm / 15 min)\n",
    "    RH2m = relative humidity (%)\n",
    "    SNOW = snow height (mm)\n",
    "    ST = Surface Temperature (°C)\n",
    "    SWD = Global Horizontal Irradiance (W/m2)\n",
    "    SWDtop = Total Solar Irradiance at the top of the atmosphere (W/m2)\n",
    "    TT2M = temperature 2 meters above the ground (°C)\n",
    "    WS100m = Wind speed at 100m from the ground (m/s)\n",
    "    WS10m = Wind speed at 10m from the ground (m/s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "we = pd.read_csv('weather_data.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping SNOW and SWDtop from the dataset\n",
    "we.drop('SNOW', axis=1, inplace=True)\n",
    "we.drop('SWDtop', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining pv production and weather data into single dataframe\n",
    "df = pd.concat([pv, we], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['PV'].plot(); plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(dataframe, resample=False):\n",
    "    df = dataframe.copy()\n",
    "    if resample:\n",
    "        df = df.resample('1H').mean()\n",
    "    \n",
    "    # Engineer features from time-series data\n",
    "    for col in df.columns:\n",
    "        for i in range(1,25):\n",
    "            # Shift data by lag of 1 to 24 hours\n",
    "            df[col+'_{:d}h'.format(i)] = df[col].shift(periods=i)  # time-lag\n",
    "        df[col+'_diff'] = df[col].diff()  # difference\n",
    "    \n",
    "    # Hour-of-day indicators with cyclical transform\n",
    "    dayhour_ind = df.index.hour\n",
    "    df['hr_sin'] = np.sin(dayhour_ind*(2.*np.pi/24))\n",
    "    df['hr_cos'] = np.cos(dayhour_ind*(2.*np.pi/24))\n",
    "    \n",
    "    # Month indicators with cyclical transform\n",
    "    month_ind = df.index.month\n",
    "    df['mnth_sin'] = np.sin((month_ind-1)*(2.*np.pi/12))\n",
    "    df['mnth_cos'] = np.cos((month_ind-1)*(2.*np.pi/12))\n",
    "    \n",
    "    # Encoding sunshine hours\n",
    "    sun_ind = df['PV'] > 0.\n",
    "    df['sun'] = sun_ind.astype(int)\n",
    "    \n",
    "    # Drop rows with NaN values\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfr = engineer_features(df, resample=True)\n",
    "dfr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def time_series_data_split(df, start_date, window_days, train_percent=0.8, print_dates=False):\n",
    "    # DATASETS DETERMINED USING THE WINDOW METHOD\n",
    "    # Preserving time ordering!\n",
    "    features = [col for col in df.columns if col != 'PV']\n",
    "    # window_days - width of the dataset window in days\n",
    "    # this window determines the size of the dataset that\n",
    "    # will be split into the training and validation sets\n",
    "    # train_percent - percent of the dataset used for training\n",
    "    \n",
    "    # Training period\n",
    "    st = pd.to_datetime(start_date, utc=True)\n",
    "    et = st + dt.timedelta(days=int(train_percent*window_days)-1)\n",
    "    X_train = df[features].loc[st:et].values\n",
    "    y_train = df['PV'].loc[st:et].values\n",
    "    \n",
    "    # Validation period\n",
    "    sv = et #+ dt.timedelta(days=1)\n",
    "    ev = sv + dt.timedelta(days=int((1-train_percent)*window_days))\n",
    "    X_test = df[features].loc[sv:ev].values  # validation\n",
    "    y_test = df['PV'].loc[sv:ev].values   # validation\n",
    "    \n",
    "    # Testing period (one day after)\n",
    "    sn = ev #+ dt.timedelta(days=1)\n",
    "    en = sn + dt.timedelta(days=1)\n",
    "    X_new = df[features].loc[sn:en].values\n",
    "    y_true = df['PV'].loc[sn:en].values\n",
    "    \n",
    "    if print_dates:\n",
    "        print('  Training period:', st.date(), '=>', et.date())\n",
    "        print('Validation period:', sv.date(), '=>', ev.date())\n",
    "        print('   Testing period:', sn.date(), '=>', en.date())\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test, X_new, y_true"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_date = '2019-05-15'  # start date\n",
    "window_days = 30\n",
    "X_train, y_train, X_test, y_test, X_new, y_true = time_series_data_split(dfr, start_date, window_days, print_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline: SelectKBest and RandomForest\n",
    "# SelectKBest is used for features reduction\n",
    "selectbest = SelectKBest(score_func=mutual_info_regression, k='all')\n",
    "forest = RandomForestRegressor(criterion='mse')\n",
    "# Creating a pipeline\n",
    "pipe = Pipeline(steps=[('kbest', selectbest), ('forest', forest)])\n",
    "# Parameters of pipeline for the randomized search with cross-validation\n",
    "param_dists = {'kbest__k': stats.randint(low=32, high=128), \n",
    "               'forest__n_estimators': stats.randint(low=200, high=1000),\n",
    "               'forest__max_depth': [1, 3, 5, None]}\n",
    "NITER = 100  # number of random search iterations\n",
    "time_start = timeit.default_timer()\n",
    "search = RandomizedSearchCV(estimator=pipe, param_distributions=param_dists, \n",
    "                            cv=TimeSeriesSplit(n_splits=3),\n",
    "                            scoring='neg_mean_squared_error',\n",
    "                            n_iter=NITER, refit=True, n_jobs=-1)\n",
    "search.fit(X_train, y_train)\n",
    "time_end = timeit.default_timer()\n",
    "time_elapsed = time_end - time_start\n",
    "print('Execution time (hour:min:sec): {}'.format(str(dt.timedelta(seconds=time_elapsed))))\n",
    "print('Best parameter (CV score = {:.3f}):'.format(search.best_score_))\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest with grid-search hyperparameter optimization\n",
    "parameters = {'n_estimators':[250, 500, 1000],\n",
    "              'max_depth':[1, 5, None]}\n",
    "time_start = timeit.default_timer()\n",
    "search = GridSearchCV(estimator=RandomForestRegressor(criterion='mse'), \n",
    "                      param_grid=parameters, cv=TimeSeriesSplit(n_splits=3),\n",
    "                      scoring='neg_mean_squared_error', \n",
    "                      refit=True, n_jobs=-1)\n",
    "search.fit(X_train, y_train)\n",
    "time_end = timeit.default_timer()\n",
    "time_elapsed = time_end - time_start\n",
    "print('Execution time (hour:min:sec): {}'.format(str(dt.timedelta(seconds=time_elapsed))))\n",
    "print('Best parameter (CV score = {:.3f}):'.format(search.best_score_))\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis with random forests\n",
    "forest = RandomForestRegressor(criterion='mse', **search.best_params_)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP = 10\n",
    "feature_importance = forest.feature_importances_\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)[-TOP:]\n",
    "pos = np.arange(sorted_idx.shape[0]) + .25\n",
    "# Plot relative feature importance\n",
    "fig, ax = plt.subplots(figsize=(7,4))\n",
    "ax.barh(pos, feature_importance[sorted_idx][-TOP:], align='center', color='magenta', alpha=0.6)\n",
    "plt.yticks(pos, dfr.columns[sorted_idx][-TOP:])\n",
    "ax.set_xlabel('Feature Relative Importance')\n",
    "ax.grid(axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for new data\n",
    "y_pred = search.predict(X_new)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(y_pred, label='prediction')\n",
    "plt.plot(y_true, lw=2, label='true value')\n",
    "plt.legend()\n",
    "plt.grid(axis='y')\n",
    "plt.xlabel('Hours')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nastavak"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All data is numeric; no need to handle text and categorical features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separating features and labels\n",
    "X = df.drop('PV', axis=1) # Features\n",
    "y = df['PV'] # Labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Currently, features are not scaled"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Selecting and training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performing train-test-split \n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using Grid Search for tuning Random Forest Regressor hyperparameters\n",
    "time_start = timeit.default_timer()\n",
    "parameters = {'n_estimators': [100, 500, 1000], 'max_depth': [1, 3, None]}\n",
    "rfr_gs = GridSearchCV(estimator=RandomForestRegressor(criterion='mse'),\n",
    "                     param_grid=parameters, cv=TimeSeriesSplit(n_splits=3),\n",
    "                     n_jobs=-1)\n",
    "rfr_gs.fit(X_train, y_train)\n",
    "print(rfr_gs.best_params_)\n",
    "time_end = timeit.default_timer()\n",
    "time_elapsed = time_end - time_start\n",
    "print('Execution time (hour:min:sec): {}'.format(str(dt.timedelta(seconds=time_elapsed))))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choosing optimal hyperparameters from Grid Search output\n",
    "rfr = RandomForestRegressor(criterion='mse', **rfr_gs.best_params_)\n",
    "rfr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Predicting on test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds = rfr.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MAE of predictions\n",
    "mean_absolute_error(y_test, y_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize the predicted labels on test set\n",
    "start = 1\n",
    "end = start + 24*4\n",
    "plt.figure(figsize=(15,8))\n",
    "plt.plot(y_preds[start:end], ls='--', label='predictions')\n",
    "plt.plot(y_test.iloc[start:end].values, label='actual')\n",
    "plt.grid()\n",
    "plt.legend();"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
