{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time-series forecasting of PV production"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import seaborn as sns\n",
    "    # Seaborn style (figure aesthetics only)\n",
    "    sns.set(context='paper', style='whitegrid', font_scale=1.2)\n",
    "    sns.set_style('ticks', {'xtick.direction':'in', 'ytick.direction':'in'})\n",
    "except ImportError:\n",
    "    print('Seaborn not installed. Going without it.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVR\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.decomposition import PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PV Data\n",
    "\n",
    "5 seconds resolution MiRIS PV from 13/05/2019 to 21/06/2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv = pd.read_csv('miris_pv.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling the dataset from 5-seconds to 15-minutes resolution (using mean)\n",
    "pv = pv.resample('15min').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15-minute resolution weather data\n",
    "\n",
    "The file is composed of forecast of several weather variables:\n",
    "\n",
    "    CD = low clouds (0 to 1)\n",
    "    CM = medium clouds (0 to 1)\n",
    "    CU = high clouds (0 to 1)\n",
    "    PREC = precipitation (mm / 15 min)\n",
    "    RH2m = relative humidity (%)\n",
    "    SNOW = snow height (mm)\n",
    "    ST = Surface Temperature (°C)\n",
    "    SWD = Global Horizontal Irradiance (W/m2)\n",
    "    SWDtop = Total Solar Irradiance at the top of the atmosphere (W/m2)\n",
    "    TT2M = temperature 2 meters above the ground (°C)\n",
    "    WS100m = Wind speed at 100m from the ground (m/s)\n",
    "    WS10m = Wind speed at 10m from the ground (m/s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "we = pd.read_csv('weather_data.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping SNOW and SWDtop from the dataset\n",
    "we.drop('SNOW', axis=1, inplace=True)\n",
    "we.drop('SWDtop', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining pv production and weather data into single dataframe\n",
    "df = pd.concat([pv, we], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['PV']].plot(figsize=(12,4.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features engineering from time-series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(dataframe, window=24, steps_ahead=1, \n",
    "                      copy_data=True, resample=True, drop_nan_rows=True):\n",
    "    \"\"\" Engineer features from time-series data\n",
    "\n",
    "    Features engineering from the time-series data by using time-shift,\n",
    "    first-differencing, rolling window statistics, cyclical transforms,\n",
    "    and encoding. NaN values are dropped from the final dataset.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    dataframe: pd.DataFrame\n",
    "        Pandas dataframe holding the original time-series data \n",
    "        (in the long table format).\n",
    "    window: int\n",
    "        Window size for the past observations (for time-shifting).\n",
    "    steps_ahead: int\n",
    "        Number of time steps ahead for multi-step forecasting \n",
    "        (steps_ahead=1 means single-step ahead forecasting).\n",
    "    copy_data: bool\n",
    "        True/False indicator for making a local copy of the dataframe\n",
    "        inside the function.\n",
    "    resample: bool\n",
    "        True/False indicator for resampling data to hourly frequency.\n",
    "   drop_nan_rows: bool\n",
    "        True/False indicator to drop rows with NaN values.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df: pd.DataFrame\n",
    "        Pandas dataframe with newly engineered features (long format).\n",
    "    \n",
    "    Raises\n",
    "    ------\n",
    "    Nothing.\n",
    "    \"\"\"\n",
    "    \n",
    "    if copy_data:\n",
    "        df = dataframe.copy()\n",
    "    else:\n",
    "        df = dataframe\n",
    "    if resample:\n",
    "        df = df.resample('1H').mean()\n",
    "    \n",
    "    # Engineer features from time-series data\n",
    "    columns = df.columns\n",
    "    for col in columns:\n",
    "        for i in range(1, window+1):\n",
    "            # Shift data by lag of 1 to window=24 hours\n",
    "            df[col+'_{:d}h'.format(i)] = df[col].shift(periods=i)  # time-lag\n",
    "    for col in columns:\n",
    "        df[col+'_diff'] = df[col].diff()  # first-difference\n",
    "    df['PV_diff24'] = df['PV'].diff(24)\n",
    "\n",
    "    # Rolling windows (24-hours) on time-shifted PV production\n",
    "    df['roll_mean'] = df['PV_1h'].rolling(window=24, win_type='hamming').mean()\n",
    "    df['roll_max'] = df['PV_1h'].rolling(window=24).max()\n",
    "    \n",
    "    # Hour-of-day indicators with cyclical transform\n",
    "    dayhour_ind = df.index.hour\n",
    "    df['hr_sin'] = np.sin(dayhour_ind*(2.*np.pi/24))\n",
    "    df['hr_cos'] = np.cos(dayhour_ind*(2.*np.pi/24))\n",
    "    \n",
    "    # Month indicators with cyclical transform\n",
    "    month_ind = df.index.month\n",
    "    df['mnth_sin'] = np.sin((month_ind-1)*(2.*np.pi/12))\n",
    "    df['mnth_cos'] = np.cos((month_ind-1)*(2.*np.pi/12))\n",
    "\n",
    "    # Encoding sunshine hours\n",
    "    sun_ind = df['PV'] > 0.\n",
    "    df['sun'] = sun_ind.astype(int)\n",
    "\n",
    "    # Forecast horizont\n",
    "    if steps_ahead == 1:\n",
    "        # Single-step ahead forecasting\n",
    "        df['PV+0h'] = df['PV'].values\n",
    "    else:\n",
    "        # Multi-step ahead forecasting\n",
    "        for i in range(steps_ahead):\n",
    "            df['PV'+'+{:d}h'.format(i)] = df['PV'].shift(-i)\n",
    "    del df['PV']\n",
    "\n",
    "    # Drop rows with NaN values\n",
    "    if drop_nan_rows:\n",
    "        df.dropna(inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Single-step model\n",
    "df2 = engineer_features(df)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, validation, and test datasets (time-series data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(dataframe, weather_forecast=False, copy_data=True):\n",
    "    if copy_data:\n",
    "        df = dataframe.copy()\n",
    "    else:\n",
    "        df = dataframe\n",
    "    if weather_forecast is False:\n",
    "        # Hour-ahead weather forecast is NOT being utilized\n",
    "        df.drop(columns=['CD', 'CM', 'CU', 'PREC', 'RH2m', 'ST', \n",
    "                        'SWD', 'TT2M', 'WS100m', 'WS10m'],\n",
    "                inplace=True)\n",
    "\n",
    "    columns = df.columns.values\n",
    "    outputs = [col_name for col_name in columns if 'PV+' in col_name]\n",
    "    inputs = [col_name for col_name in columns if col_name not in outputs]\n",
    "    # inputs (features)\n",
    "    X = df[inputs]\n",
    "    # outputs\n",
    "    y = df[outputs]\n",
    "    return X, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hour-ahead weather forecast is NOT being utilized\n",
    "weather_forecast = False\n",
    "# Prepare dataframe for a split into train, test sets\n",
    "X, y = prepare_data(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test dataset split (w/o shuffle)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print train and test set shapes\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-step model pipeline with features selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = 'SVR'  # 'RandomForest'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "if model == 'RandomForest':\n",
    "    # Pipeline: SelectKBest and RandomForest\n",
    "    # SelectKBest is used for features selection/reduction\n",
    "    selectbest = SelectKBest(score_func=mutual_info_regression, k='all')\n",
    "    forest = RandomForestRegressor(criterion='mse', bootstrap=True)\n",
    "    # Creating a pipeline\n",
    "    pipe = Pipeline(steps=[('preprocess', 'passthrough'), \n",
    "                           ('kbest', selectbest), \n",
    "                           ('forest', forest)])\n",
    "    # Parameters of pipeline for the randomized search with cross-validation\n",
    "    param_dists = {'preprocess': [None, StandardScaler()], \n",
    "                   'kbest__k': stats.randint(low=32, high=128), \n",
    "                   'forest__n_estimators': stats.randint(low=200, high=1000),\n",
    "                   'forest__max_depth': [1, 3, 5, None], \n",
    "                   'forest__max_samples': stats.uniform(loc=0.2, scale=0.8),\n",
    "                  }\n",
    "elif model == 'SVR':\n",
    "    # Pipeline: SelectKBest and SVR\n",
    "    # SelectKBest is used for features selection/reduction\n",
    "    selectbest = SelectKBest(score_func=mutual_info_regression, k='all')\n",
    "    # Support Vector Regression \n",
    "    svr = SVR(kernel='rbf', gamma='scale')\n",
    "    # Creating a pipeline\n",
    "    pipe = Pipeline(steps=[('preprocess', 'passthrough'), \n",
    "                           ('kbest', selectbest), \n",
    "                           ('svr', svr)])\n",
    "    # Parameters of pipeline for the randomized search with cross-validation\n",
    "    param_dists = {'preprocess': [None, StandardScaler()], \n",
    "                   'kbest__k': stats.randint(low=32, high=128), \n",
    "                   'svr__C': stats.loguniform(1e0, 1e3),\n",
    "                   'svr__epsilon': stats.loguniform(1e-5, 1e-2),\n",
    "                  }\n",
    "else:\n",
    "    raise NotImplementedError('Model name \"{}\" is not recognized or implemented!'.format(model))\n",
    "\n",
    "NITER = 100  # number of random search iterations\n",
    "time_start = timeit.default_timer()\n",
    "search = RandomizedSearchCV(estimator=pipe, param_distributions=param_dists, \n",
    "                            cv=TimeSeriesSplit(n_splits=3),\n",
    "                            scoring='neg_mean_squared_error',\n",
    "                            n_iter=NITER, refit=True, n_jobs=-1)\n",
    "search.fit(X_train, y_train)\n",
    "time_end = timeit.default_timer()\n",
    "time_elapsed = time_end - time_start\n",
    "print('Execution time (hour:min:sec): {}'.format(str(dt.timedelta(seconds=time_elapsed))))\n",
    "print('Best parameter (CV score = {:.3f}):'.format(search.best_score_))\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Feature importance analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if model == 'RandomForest':\n",
    "    # Feature importance analysis with random forests\n",
    "    best_params = {'n_estimators': search.best_params_['forest__n_estimators'],\n",
    "                   'max_depth': search.best_params_['forest__max_depth'],\n",
    "                   'max_samples': search.best_params_['forest__max_samples'],\n",
    "                  }\n",
    "    forest = RandomForestRegressor(criterion='mse', **best_params)\n",
    "    forest.fit(X_train, y_train)\n",
    "\n",
    "    TOP = 15\n",
    "    feature_importance = forest.feature_importances_\n",
    "    feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "    sorted_idx = np.argsort(feature_importance)[-TOP:]\n",
    "    pos = np.arange(sorted_idx.shape[0]) + .25\n",
    "\n",
    "    # Plot relative feature importance\n",
    "    fig, ax = plt.subplots(figsize=(7,5))\n",
    "    ax.barh(pos, feature_importance[sorted_idx][-TOP:], \n",
    "            align='center', color='magenta', alpha=0.6)\n",
    "    plt.yticks(pos, X_train.columns[sorted_idx][-TOP:])\n",
    "    ax.set_xlabel('Feature Relative Importance')\n",
    "    ax.grid(axis='x')\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Single-step ahead prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make single-step predictions for 24 hours ahead\n",
    "y_preds = search.predict(X_test.values[:24,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test.values[:24], y_preds)\n",
    "print('MSE:', mse.round(5))\n",
    "mae = mean_absolute_error(y_test.values[:24], y_preds)\n",
    "print('MAE:', mae.round(5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(y_test.index[:24], y_test.values[0:24], lw=2, label='true values')\n",
    "plt.plot(y_test.index[:24], y_preds, ls='--', lw=1.5, marker='+', ms=10, label='predictions')\n",
    "plt.text(y_test.index[20], 0.35, 'MAE: {:.3f}'.format(mae), horizontalalignment='center', fontweight='bold')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(axis='y')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Day/Hour')\n",
    "plt.ylabel('PV power')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walk-forward multi-step prediction with a single-step model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if weather_forecast:\n",
    "    raise NotImplementedError('Walk forward is not implemented with hour-ahead weather forecast.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WALK = 12  # walk-forward for WALK hours\n",
    "STEP = 24  # multi-step predict for STEP hours ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# With STEP=24 and WALK=12, we are making a 24-hour ahead predictions \n",
    "# after each hour, and move forward in time for 12 hours in total. \n",
    "# In other words, we walk forward for 12 hours, and each time we move \n",
    "# forward (by one hour) we make a brand new 24-hour ahead predictions. \n",
    "# Predicted values are being utilized as past observations for making\n",
    "# new predictions as we walk forward in time. Hence, as we move away in \n",
    "# time from the present moment we are relying more and more on predicted \n",
    "# values to make new predictions!\n",
    "\n",
    "def walk_forward(X_values, y_predicted, window=24):\n",
    "    \"\"\" Walk forward\n",
    "\n",
    "    Preparing input matrix X for walk-forward single-step predictions.\n",
    "    There are eleven different original variables (PV plus 10 weather \n",
    "    vars.), which have been time-shifted using the \"window\" method.\n",
    "    NOTE: This function uses certain hard-coded elements adjusted for \n",
    "    the particular problem at hand.\n",
    "\n",
    "    Arguments\n",
    "    ---------\n",
    "    X_values: np.array\n",
    "        Input values for making predictions.\n",
    "    Y_predicted: float\n",
    "        Predicted value.\n",
    "    window: int\n",
    "        Window size of the past observations. It should be equal to \n",
    "        the window size that was used for features engineering.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    X_values: np.array\n",
    "        Input values time-shifted by a single time step, using the \n",
    "        walk forward approach.\n",
    "\n",
    "    Raises\n",
    "    ------\n",
    "    Nothing.\n",
    "    \"\"\"\n",
    "\n",
    "    # There are eleven different original\n",
    "    # variables (PV plus 10 weather vars)\n",
    "    X_parts = []\n",
    "    j = 0; k = 0\n",
    "    for i in range(11):\n",
    "        k = j + window\n",
    "        X_part = X_values[j:k]\n",
    "        X_part = pd.Series(X_part)\n",
    "        if i == 0:\n",
    "            # time-shifted PV production\n",
    "            X_part = X_part.shift(periods=1, fill_value=y_predicted)\n",
    "        else:\n",
    "            # time-shifted weather features\n",
    "            X_part = X_part.shift(periods=1, fill_value=np.NaN)\n",
    "            X_part.fillna(method='bfill', inplace=True)  # back-fill\n",
    "        X_parts.append(X_part.values)\n",
    "        j += window\n",
    "    X_parts = np.asarray(X_parts).reshape(1,-1).flatten()\n",
    "    X_rest = X_values[-19:]   # other features (hard-coded)\n",
    "    # Update feature PV_diff with y_predicted\n",
    "    X_rest[0] = X_parts[0] - X_parts[1]\n",
    "    # Concatenate\n",
    "    X_values = np.r_[X_parts, X_rest]\n",
    "    return X_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_predictions(walk, y_test, y_pred):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.title('walk forward +{:2d} hours'.format(walk+1))\n",
    "    plt.plot(y_test.values[walk:walk+STEP], lw=2.5, label='true values')\n",
    "    plt.plot(y_pred, ls='--', lw=1.5, marker='+', ms=10, label='predictions')\n",
    "    mae = mean_absolute_error(y_test.values[walk:walk+STEP], y_pred)\n",
    "    plt.text(STEP-2, 0.35, 'MAE: {:.3f}'.format(mae), \n",
    "             horizontalalignment='right', \n",
    "             fontweight='bold')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylim(top=0.5)\n",
    "    plt.grid(axis='y')\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('PV power')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Do walk-forward predictions\n",
    "for k in range(WALK):\n",
    "    X_test_values = X_test.values[k,:]\n",
    "    y_pred_values = []\n",
    "    for i in range(STEP):\n",
    "        # Predict next time-step value\n",
    "        y_predict = search.predict(X_test_values.reshape(1,-1))[0]\n",
    "        y_pred_values.append(y_predict)\n",
    "        # Walk-forward for a single time step\n",
    "        X_test_values = walk_forward(X_test_values, y_predict)\n",
    "    # Plot walk-forward predictions against true values\n",
    "    plot_predictions(k, y_test, y_pred_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multi-step model pipeline without features selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-step model (24-hours ahead)\n",
    "df2 = engineer_features(df, steps_ahead=STEP)\n",
    "# Prepare dataframe for a split into train, test sets\n",
    "X, y = prepare_data(df2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test dataset split (w/o shuffle)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print train and test set shapes\n",
    "print(X_train.shape, y_train.shape)\n",
    "print(X_test.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_model = 'SVR'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if multi_model == 'RandomForest':\n",
    "    # Random Forest Regression (supports multi-output natively)\n",
    "    forest = RandomForestRegressor(criterion='mse', bootstrap=True)\n",
    "    # Creating a pipeline\n",
    "    pipe = Pipeline(steps=[('preprocess', 'passthrough'), \n",
    "                           ('forest', forest)])\n",
    "    # Parameters of pipeline for the randomized search with cross-validation\n",
    "    param_dists = {'preprocess': [None, StandardScaler()], \n",
    "                   'forest__n_estimators': stats.randint(low=200, high=1000),\n",
    "                   'forest__max_depth': [1, 3, 5, None], \n",
    "                   'forest__max_samples': stats.uniform(loc=0.2, scale=0.8),\n",
    "                  }\n",
    "elif multi_model == 'SVR':\n",
    "    # Support Vector Regression (does NOT support multi-output natively)\n",
    "    svr = MultiOutputRegressor(SVR(kernel='rbf', gamma='scale'))\n",
    "    # Creating a pipeline\n",
    "    pipe = Pipeline(steps=[('preprocess', 'passthrough'), \n",
    "                           ('svr', svr)])\n",
    "    # Parameters of pipeline for the randomized search with cross-validation\n",
    "    param_dists = {'preprocess': [None, StandardScaler()], \n",
    "                   'svr__estimator__C': stats.loguniform(1e0, 1e3),\n",
    "                   'svr__estimator__epsilon': stats.loguniform(1e-5, 1e-2),\n",
    "                  }\n",
    "elif multi_model == 'PCA+SVR':\n",
    "    # Principal Component Analysis (PCA) is used for decomposing \n",
    "    # (i.e. projecting) features into the lower-dimensional space\n",
    "    # while retaining maximum amount of the variance.\n",
    "    pca = PCA(whiten=True, svd_solver='full')\n",
    "    # Support Vector Regression (does NOT support multi-output natively)\n",
    "    svr = MultiOutputRegressor(SVR(kernel='rbf', gamma='scale'))\n",
    "    # Creating a pipeline\n",
    "    pipe = Pipeline(steps=[('pca', pca), \n",
    "                           ('svr', svr)])\n",
    "    # Parameters of pipeline for the randomized search with cross-validation\n",
    "    param_dists = {'pca__n_components': stats.uniform(),\n",
    "                   'svr__estimator__C': stats.loguniform(1e0, 1e3),\n",
    "                   'svr__estimator__epsilon': stats.loguniform(1e-5, 1e-2),\n",
    "                  }\n",
    "else:\n",
    "    raise NotImplementedError('Model name \"{}\" is not recognized or implemented!'.format(model))\n",
    "\n",
    "NITER = 100  # number of random search iterations\n",
    "time_start = timeit.default_timer()\n",
    "search_multi = RandomizedSearchCV(estimator=pipe, param_distributions=param_dists, \n",
    "                                  cv=TimeSeriesSplit(n_splits=3),\n",
    "                                  scoring='neg_mean_squared_error',\n",
    "                                  n_iter=NITER, refit=True, n_jobs=-1)\n",
    "search_multi.fit(X_train, y_train)\n",
    "time_end = timeit.default_timer()\n",
    "time_elapsed = time_end - time_start\n",
    "print('Execution time (hour:min:sec): {}'.format(str(dt.timedelta(seconds=time_elapsed))))\n",
    "print('Best parameter (CV score = {:.3f}):'.format(search_multi.best_score_))\n",
    "print(search_multi.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_multi_step_predictions(walk, y_test, y_pred):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.title('walk forward +{:2d} hours'.format(walk+1))\n",
    "    plt.plot(y_test, lw=2.5, label='true values')\n",
    "    plt.plot(y_pred, ls='--', lw=1.5, marker='+', ms=10, label='predictions')\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    plt.text(STEP-2, 0.35, 'MAE: {:.3f}'.format(mae), \n",
    "             horizontalalignment='right', \n",
    "             fontweight='bold')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylim(top=0.5)\n",
    "    plt.grid(axis='y')\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('PV power')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do multi-step ahead predictions\n",
    "for k in range(WALK):\n",
    "    X_test_values = X_test.values[k+20,:]  # +20 hard-coded shift to align views with that\n",
    "    y_test_values = y_test.values[k+20,:]  # of walk-forward predictions for easy comparison\n",
    "    y_predict = search_multi.predict(X_test_values.reshape(1,-1)).flatten()\n",
    "    # Plot multi-step predictions against true values\n",
    "    plot_multi_step_predictions(k, y_test_values, y_predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}