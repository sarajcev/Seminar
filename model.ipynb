{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import seaborn as sns\n",
    "    # Seaborn style (figure aesthetics only)\n",
    "    sns.set(context='paper', style='whitegrid', font_scale=1.2)\n",
    "    sns.set_style('ticks', {'xtick.direction':'in', 'ytick.direction':'in'})\n",
    "except ImportError:\n",
    "    print('Seaborn not installed. Going without it.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PV Data\n",
    "\n",
    "5 seconds resolution MiRIS PV from 13/05/2019 to 21/06/2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv = pd.read_csv('miris_pv.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling the dataset from 5-seconds to 15-minutes resolution (using mean)\n",
    "pv = pv.resample('15min').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15-minute resolution weather data\n",
    "\n",
    "The file is composed of forecast of several weather variables:\n",
    "\n",
    "    CD = low clouds (0 to 1)\n",
    "    CM = medium clouds (0 to 1)\n",
    "    CU = high clouds (0 to 1)\n",
    "    PREC = precipitation (mm / 15 min)\n",
    "    RH2m = relative humidity (%)\n",
    "    SNOW = snow height (mm)\n",
    "    ST = Surface Temperature (°C)\n",
    "    SWD = Global Horizontal Irradiance (W/m2)\n",
    "    SWDtop = Total Solar Irradiance at the top of the atmosphere (W/m2)\n",
    "    TT2M = temperature 2 meters above the ground (°C)\n",
    "    WS100m = Wind speed at 100m from the ground (m/s)\n",
    "    WS10m = Wind speed at 10m from the ground (m/s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "we = pd.read_csv('weather_data.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping SNOW and SWDtop from the dataset\n",
    "we.drop('SNOW', axis=1, inplace=True)\n",
    "we.drop('SWDtop', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining pv production and weather data into single dataframe\n",
    "df = pd.concat([pv, we], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['PV']].plot(figsize=(12,4.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features engineering from time-series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(dataframe, copy_data=True, resample=True):\n",
    "    if copy_data:\n",
    "        df = dataframe.copy()\n",
    "    if resample:\n",
    "        df = df.resample('1H').mean()\n",
    "    \n",
    "    # Engineer features from time-series data\n",
    "    for col in df.columns:\n",
    "        for i in range(1,25):\n",
    "            # Shift data by lag of 1 to 24 hours\n",
    "            df[col+'_{:d}h'.format(i)] = df[col].shift(periods=i)  # time-lag\n",
    "        df[col+'_diff'] = df[col].diff()  # difference\n",
    "    \n",
    "    # Rolling windows on time-shifted PV production\n",
    "    df['roll_mean'] = df['PV_1h'].rolling(window=24, win_type='hamming').mean()\n",
    "    df['roll_max'] = df['PV_1h'].rolling(window=24).max()\n",
    "    \n",
    "    # Hour-of-day indicators with cyclical transform\n",
    "    dayhour_ind = df.index.hour\n",
    "    df['hr_sin'] = np.sin(dayhour_ind*(2.*np.pi/24))\n",
    "    df['hr_cos'] = np.cos(dayhour_ind*(2.*np.pi/24))\n",
    "    \n",
    "    # Month indicators with cyclical transform\n",
    "    month_ind = df.index.month\n",
    "    df['mnth_sin'] = np.sin((month_ind-1)*(2.*np.pi/12))\n",
    "    df['mnth_cos'] = np.cos((month_ind-1)*(2.*np.pi/12))\n",
    "\n",
    "    # Encoding sunshine hours\n",
    "    sun_ind = df['PV'] > 0.\n",
    "    df['sun'] = sun_ind.astype(int)\n",
    "    \n",
    "    # Drop rows with NaN values\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = engineer_features(df)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, validation, and test datasets (time-series data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vremenska_prognoza = False\n",
    "\n",
    "if vremenska_prognoza:\n",
    "    # Koristi se vremenska prognoza za sat unaprijed\n",
    "    y = df2['PV']\n",
    "    X = df2.drop('PV', axis=1)\n",
    "else:\n",
    "    # NE koristi se vremenska prognoza za sat unaprijed\n",
    "    y = df2['PV']\n",
    "    X = df2.drop(columns=['PV', 'CD', 'CM', 'CU', 'PREC', 'RH2m', \n",
    "                          'ST', 'SWD', 'TT2M', 'WS100m', 'WS10m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test dataset split (w/o shuffle)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle=False)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL: Pipeline from SelectKBest and RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pipeline: SelectKBest and RandomForest\n",
    "# SelectKBest is used for features reduction\n",
    "selectbest = SelectKBest(score_func=mutual_info_regression, k='all')\n",
    "forest = RandomForestRegressor(criterion='mse', bootstrap=True)\n",
    "# Creating a pipeline\n",
    "pipe = Pipeline(steps=[('preprocess', 'passthrough'), \n",
    "                       ('kbest', selectbest), \n",
    "                       ('forest', forest)])\n",
    "# Parameters of pipeline for the randomized search with cross-validation\n",
    "param_dists = {'preprocess': [None, StandardScaler()], \n",
    "               'kbest__k': stats.randint(low=32, high=128), \n",
    "               'forest__n_estimators': stats.randint(low=200, high=1000),\n",
    "               'forest__max_depth': [1, 3, 5, None], \n",
    "               'forest__max_samples': stats.uniform(loc=0.2, scale=0.8),\n",
    "               }\n",
    "NITER = 100  # number of random search iterations\n",
    "time_start = timeit.default_timer()\n",
    "search = RandomizedSearchCV(estimator=pipe, param_distributions=param_dists, \n",
    "                            cv=TimeSeriesSplit(n_splits=3),\n",
    "                            scoring='neg_mean_squared_error',\n",
    "                            n_iter=NITER, refit=True, n_jobs=-1)\n",
    "search.fit(X_train, y_train)\n",
    "time_end = timeit.default_timer()\n",
    "time_elapsed = time_end - time_start\n",
    "print('Execution time (hour:min:sec): {}'.format(str(dt.timedelta(seconds=time_elapsed))))\n",
    "print('Best parameter (CV score = {:.3f}):'.format(search.best_score_))\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis with random forests\n",
    "best_params = {'n_estimators': search.best_params_['forest__n_estimators'],\n",
    "               'max_depth': search.best_params_['forest__max_depth']}\n",
    "forest = RandomForestRegressor(criterion='mse', **best_params)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP = 15\n",
    "feature_importance = forest.feature_importances_\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)[-TOP:]\n",
    "pos = np.arange(sorted_idx.shape[0]) + .25\n",
    "\n",
    "# Plot relative feature importance\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "ax.barh(pos, feature_importance[sorted_idx][-TOP:], align='center', color='magenta', alpha=0.6)\n",
    "plt.yticks(pos, X_train.columns[sorted_idx][-TOP:])\n",
    "ax.set_xlabel('Feature Relative Importance')\n",
    "ax.grid(axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions for new data\n",
    "y_preds = search.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test, y_preds)\n",
    "print('MSE:', mse)\n",
    "mae = mean_absolute_error(y_test, y_preds)\n",
    "print('MAE:', mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,4.5))\n",
    "plt.plot(y_test.index, y_preds, label='prediction')\n",
    "plt.plot(y_test.index, y_test.values, lw=2, label='true value')\n",
    "plt.legend()\n",
    "plt.grid(axis='y')\n",
    "plt.xlabel('Days/Hours')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}