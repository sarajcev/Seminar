{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import timeit\n",
    "import datetime as dt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import seaborn as sns\n",
    "    # Seaborn style (figure aesthetics only)\n",
    "    sns.set(context='paper', style='whitegrid', font_scale=1.2)\n",
    "    sns.set_style('ticks', {'xtick.direction':'in', 'ytick.direction':'in'})\n",
    "except ImportError:\n",
    "    print('Seaborn not installed. Going without it.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import TimeSeriesSplit\n",
    "from sklearn.model_selection import GridSearchCV, RandomizedSearchCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error\n",
    "from sklearn.feature_selection import SelectKBest, mutual_info_regression\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PV Data\n",
    "\n",
    "5 seconds resolution MiRIS PV from 13/05/2019 to 21/06/2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pv = pd.read_csv('miris_pv.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resampling the dataset from 5-seconds to 15-minutes resolution (using mean)\n",
    "pv = pv.resample('15min').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weather Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15-minute resolution weather data\n",
    "\n",
    "The file is composed of forecast of several weather variables:\n",
    "\n",
    "    CD = low clouds (0 to 1)\n",
    "    CM = medium clouds (0 to 1)\n",
    "    CU = high clouds (0 to 1)\n",
    "    PREC = precipitation (mm / 15 min)\n",
    "    RH2m = relative humidity (%)\n",
    "    SNOW = snow height (mm)\n",
    "    ST = Surface Temperature (°C)\n",
    "    SWD = Global Horizontal Irradiance (W/m2)\n",
    "    SWDtop = Total Solar Irradiance at the top of the atmosphere (W/m2)\n",
    "    TT2M = temperature 2 meters above the ground (°C)\n",
    "    WS100m = Wind speed at 100m from the ground (m/s)\n",
    "    WS10m = Wind speed at 10m from the ground (m/s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "we = pd.read_csv('weather_data.csv', index_col=0, parse_dates=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cleaning data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dropping SNOW and SWDtop from the dataset\n",
    "we.drop('SNOW', axis=1, inplace=True)\n",
    "we.drop('SWDtop', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Joining pv production and weather data into single dataframe\n",
    "df = pd.concat([pv, we], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['PV']].plot(figsize=(12,4.5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Features engineering from time-series data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def engineer_features(dataframe, window=24, copy_data=True, resample=True):\n",
    "    if copy_data:\n",
    "        df = dataframe.copy()\n",
    "    if resample:\n",
    "        df = df.resample('1H').mean()\n",
    "    \n",
    "    # Engineer features from time-series data\n",
    "    columns = df.columns\n",
    "    for col in columns:\n",
    "        for i in range(1, window+1):\n",
    "            # Shift data by lag of 1 to window=24 hours\n",
    "            df[col+'_{:d}h'.format(i)] = df[col].shift(periods=i)  # time-lag\n",
    "    for col in columns:\n",
    "        df[col+'_diff'] = df[col].diff()  # first-difference\n",
    "    \n",
    "    # Rolling windows (24-hours) on time-shifted PV production\n",
    "    df['roll_mean'] = df['PV_1h'].rolling(window=24, win_type='hamming').mean()\n",
    "    df['roll_max'] = df['PV_1h'].rolling(window=24).max()\n",
    "    \n",
    "    # Hour-of-day indicators with cyclical transform\n",
    "    dayhour_ind = df.index.hour\n",
    "    df['hr_sin'] = np.sin(dayhour_ind*(2.*np.pi/24))\n",
    "    df['hr_cos'] = np.cos(dayhour_ind*(2.*np.pi/24))\n",
    "    \n",
    "    # Month indicators with cyclical transform\n",
    "    month_ind = df.index.month\n",
    "    df['mnth_sin'] = np.sin((month_ind-1)*(2.*np.pi/12))\n",
    "    df['mnth_cos'] = np.cos((month_ind-1)*(2.*np.pi/12))\n",
    "\n",
    "    # Encoding sunshine hours\n",
    "    sun_ind = df['PV'] > 0.\n",
    "    df['sun'] = sun_ind.astype(int)\n",
    "    \n",
    "    # Drop rows with NaN values\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = engineer_features(df)\n",
    "df2.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, validation, and test datasets (time-series data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "weather_forecast = False\n",
    "\n",
    "if weather_forecast:\n",
    "    # Hour-ahead weather forecast is being utilized\n",
    "    y = df2['PV']\n",
    "    X = df2.drop('PV', axis=1)\n",
    "else:\n",
    "    # Hour-ahead weather forecast is NOT being utilized\n",
    "    y = df2['PV']\n",
    "    X = df2.drop(columns=['PV', 'CD', 'CM', 'CU', 'PREC', 'RH2m', \n",
    "                          'ST', 'SWD', 'TT2M', 'WS100m', 'WS10m'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train and test dataset split (w/o shuffle)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, shuffle=False)\n",
    "print(X_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MODEL: Pipeline from SelectKBest and RandomForest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Pipeline: SelectKBest and RandomForest\n",
    "# SelectKBest is used for features reduction\n",
    "selectbest = SelectKBest(score_func=mutual_info_regression, k='all')\n",
    "forest = RandomForestRegressor(criterion='mse', bootstrap=True)\n",
    "# Creating a pipeline\n",
    "pipe = Pipeline(steps=[('preprocess', 'passthrough'), \n",
    "                       ('kbest', selectbest), \n",
    "                       ('forest', forest)])\n",
    "# Parameters of pipeline for the randomized search with cross-validation\n",
    "param_dists = {'preprocess': [None, StandardScaler()], \n",
    "               'kbest__k': stats.randint(low=32, high=128), \n",
    "               'forest__n_estimators': stats.randint(low=200, high=1000),\n",
    "               'forest__max_depth': [1, 3, 5, None], \n",
    "               'forest__max_samples': stats.uniform(loc=0.2, scale=0.8),\n",
    "               }\n",
    "NITER = 100  # number of random search iterations\n",
    "time_start = timeit.default_timer()\n",
    "search = RandomizedSearchCV(estimator=pipe, param_distributions=param_dists, \n",
    "                            cv=TimeSeriesSplit(n_splits=3),\n",
    "                            scoring='neg_mean_squared_error',\n",
    "                            n_iter=NITER, refit=True, n_jobs=-1)\n",
    "search.fit(X_train, y_train)\n",
    "time_end = timeit.default_timer()\n",
    "time_elapsed = time_end - time_start\n",
    "print('Execution time (hour:min:sec): {}'.format(str(dt.timedelta(seconds=time_elapsed))))\n",
    "print('Best parameter (CV score = {:.3f}):'.format(search.best_score_))\n",
    "print(search.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature importance analysis with random forests\n",
    "best_params = {'n_estimators': search.best_params_['forest__n_estimators'],\n",
    "               'max_depth': search.best_params_['forest__max_depth'],\n",
    "               'max_samples': search.best_params_['forest__max_samples'],\n",
    "               }\n",
    "forest = RandomForestRegressor(criterion='mse', **best_params)\n",
    "forest.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOP = 15\n",
    "feature_importance = forest.feature_importances_\n",
    "feature_importance = 100.0 * (feature_importance / feature_importance.max())\n",
    "sorted_idx = np.argsort(feature_importance)[-TOP:]\n",
    "pos = np.arange(sorted_idx.shape[0]) + .25\n",
    "\n",
    "# Plot relative feature importance\n",
    "fig, ax = plt.subplots(figsize=(7,5))\n",
    "ax.barh(pos, feature_importance[sorted_idx][-TOP:], align='center', color='magenta', alpha=0.6)\n",
    "plt.yticks(pos, X_train.columns[sorted_idx][-TOP:])\n",
    "ax.set_xlabel('Feature Relative Importance')\n",
    "ax.grid(axis='x')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make single-step predictions for 24 hours ahead\n",
    "y_preds = search.predict(X_test.values[:24,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "mse = mean_squared_error(y_test.values[:24], y_preds)\n",
    "print('MSE:', mse)\n",
    "mae = mean_absolute_error(y_test.values[:24], y_preds)\n",
    "print('MAE:', mae)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(6,4))\n",
    "plt.plot(y_test.index[:24], y_test.values[0:24], lw=2, label='true values')\n",
    "plt.plot(y_test.index[:24], y_preds, ls='--', lw=1.5, marker='+', ms=10, label='predictions')\n",
    "plt.text(y_test.index[20], 0.35, 'MAE: {:.3f}'.format(mae), horizontalalignment='center', fontweight='bold')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(axis='y')\n",
    "plt.xticks(rotation=45)\n",
    "plt.xlabel('Day/Hour')\n",
    "plt.ylabel('PV power')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "source": [
    "### Walk-forward multi-step prediction"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "WALK = 12  # walk-forward for WALK hours\n",
    "STEP = 24  # multi-step predict for STEP hours ahead"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# With STEP=24 and WALK=12, we are making a 24-hour ahead predictions \n",
    "# after each hour, and move forward in time for 12 hours in total. \n",
    "# In other words, we walk forward for 12 hours, and each time we move \n",
    "# forward (by one hour) we make a brand new 24-hour ahead predictions. \n",
    "# Predicted values are being utilized as past observations for making\n",
    "# new predictions as we walk forward in time. Hence, as we move away in \n",
    "# time from the present moment we are relying more and more on predicted \n",
    "# values to make new predictions!\n",
    "\n",
    "def walk_forward(X_values, y_predicted, window=24):\n",
    "    # There are eleven different original\n",
    "    # variables (PV plus 10 weather vars)\n",
    "    X_parts = []\n",
    "    j = 0; k = 0\n",
    "    for i in range(11):\n",
    "        k = j + window\n",
    "        X_part = X_values[j:k]\n",
    "        X_part = pd.Series(X_part)\n",
    "        if i == 0:\n",
    "            # time-shifted PV production\n",
    "            X_part = X_part.shift(periods=1, fill_value=y_predicted)\n",
    "        else:\n",
    "            # time-shifted weather features\n",
    "            X_part = X_part.shift(periods=1, fill_value=np.NaN)\n",
    "            X_part.fillna(method='bfill', inplace=True)  # back-fill\n",
    "        X_parts.append(X_part.values)\n",
    "        j += window\n",
    "    X_parts = np.asarray(X_parts).reshape(1,-1)\n",
    "    X_rest = X_values[-18:]   # other features\n",
    "    X_values = np.r_[X_parts[0], X_rest]\n",
    "    return X_values\n",
    "\n",
    "def plot_predictions(walk, y_test, y_pred):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.title('walk forward +{:2d} hours'.format(walk+1))\n",
    "    plt.plot(y_test.values[walk:walk+STEP], lw=2.5, label='true values')\n",
    "    plt.plot(y_pred, ls='--', lw=1.5, marker='+', ms=10, label='predictions')\n",
    "    mae = mean_absolute_error(y_test.values[walk:walk+STEP], y_pred)\n",
    "    plt.text(STEP-2, 0.35, 'MAE: {:.3f}'.format(mae), \n",
    "             horizontalalignment='right', \n",
    "             fontweight='bold')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.ylim(top=0.5)\n",
    "    plt.grid(axis='y')\n",
    "    plt.xlabel('Hour')\n",
    "    plt.ylabel('PV power')\n",
    "    plt.show()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for k in range(WALK):\n",
    "    X_test_values = X_test.values[k,:]\n",
    "    y_pred_values = []\n",
    "    for i in range(STEP):\n",
    "        # Predict next time-step value\n",
    "        y_predict = search.predict(X_test_values.reshape(1,-1))[0]\n",
    "        y_pred_values.append(y_predict)\n",
    "        # Walk-forward for a single time step\n",
    "        X_test_values = walk_forward(X_test_values, y_predict)\n",
    "    # Plot walk-forward predictions against true values\n",
    "    plot_predictions(k, y_test, y_pred_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}